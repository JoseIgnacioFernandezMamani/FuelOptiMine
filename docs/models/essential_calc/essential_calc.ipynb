{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd46185b-d1d9-406c-9170-9f3a59bdee50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Essential calculus\n",
    "\n",
    "**Roadmap Matemático para Modelado de Machine Learning (ML)**  \n",
    "*(En paréntesis: modelos/áreas donde se aplica cada concepto)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Álgebra Lineal**  \n",
    "1.1 **Vectores y Espacios Vectoriales**  \n",
    "- Normas (L1, L2, L∞) *(Regularización en regresión lineal, XGBoost)*  \n",
    "- Producto punto y proyecciones *(Regresión lineal, SVM)*  \n",
    "1.2 **Matrices y Operaciones**  \n",
    "- Multiplicación de matrices *(Redes neuronales, PCA)*  \n",
    "- Inversa, transpuesta, traza *(Mínimos cuadrados, regresión lineal)*  \n",
    "- Descomposición LU/SVD *(PCA, reducción de dimensionalidad)*  \n",
    "1.3 **Autovalores y Autovectores**  \n",
    "- Diagonalización *(Análisis de componentes principales - PCA)*  \n",
    "1.4 **Sistemas de Ecuaciones Lineales**  \n",
    "- Forma matricial \\( Ax = b \\) *(Regresión lineal, optimización)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Cálculo Diferencial e Integral**  \n",
    "2.1 **Funciones y Límites**  \n",
    "- Continuidad y diferenciabilidad *(Gradientes en ML)*  \n",
    "2.2 **Derivadas**  \n",
    "- Derivadas parciales *(Gradiente descendente, redes neuronales)*  \n",
    "- Regla de la cadena *(Backpropagation en redes neuronales)*  \n",
    "2.3 **Optimización**  \n",
    "- Gradiente descendente *(Regresión lineal, XGBoost, redes neuronales)*  \n",
    "- Hessiana y métodos de segundo orden *(Optimización avanzada en XGBoost)*  \n",
    "2.4 **Integrales**  \n",
    "- Integrales múltiples *(Teoría de probabilidad, Bayes)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Estadística y Probabilidad**  \n",
    "3.1 **Distribuciones de Probabilidad**  \n",
    "- Normal, binomial, Poisson *(Modelos lineales generalizados - GLM)*  \n",
    "- Función de densidad (PDF) y masa (PMF) *(Naive Bayes, evaluación de modelos)*  \n",
    "3.2 **Teoría de Muestreo**  \n",
    "- Ley de los grandes números *(Validación de modelos)*  \n",
    "- Teorema del límite central *(Intervalos de confianza, bootstrapping)*  \n",
    "3.3 **Inferencia Estadística**  \n",
    "- Intervalos de confianza *(Validación de hipótesis)*  \n",
    "- Pruebas t y chi-cuadrado *(Selección de características)*  \n",
    "3.4 **Teorema de Bayes**  \n",
    "- Probabilidad condicional *(Naive Bayes, redes Bayesianas)*  \n",
    "- Likelihood y máxima verosimilitud *(Regresión logística, EM algorithm)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Optimización Matemática**  \n",
    "4.1 **Optimización No Restringida**  \n",
    "- Mínimos cuadrados *(Regresión lineal)*  \n",
    "- Gradiente descendente estocástico *(Redes neuronales, XGBoost)*  \n",
    "4.2 **Optimización Restringida**  \n",
    "- Programación lineal (LP) *(MILP, asignación de recursos)*  \n",
    "- Lagrangianos y KKT *(SVM, MILP)*  \n",
    "4.3 **Programación Entera Mixta (MILP)**  \n",
    "- Variables binarias/enteras *(Problemas combinatorios, logística)*  \n",
    "- Algoritmos branch-and-bound *(Resolución de MILP)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Teoría de Conjuntos y Lógica**  \n",
    "5.1 **Álgebra de Boole**  \n",
    "- Operadores lógicos (AND, OR, NOT) *(Árboles de decisión, reglas en ML)*  \n",
    "5.2 **Conjuntos y Subconjuntos**  \n",
    "- Uniones, intersecciones *(Preprocesamiento de datos, clustering)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. Matemáticas Discretas**  \n",
    "6.1 **Grafos y Árboles**  \n",
    "- Árboles de decisión *(CART, XGBoost)*  \n",
    "- Recorridos (BFS, DFS) *(Optimización en árboles)*  \n",
    "6.2 **Combinatoria**  \n",
    "- Permutaciones y combinaciones *(Selección de características, ensamblado de modelos)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **7. Geometría Analítica**  \n",
    "7.1 **Hiperplanos y Distancias**  \n",
    "- Ecuación de un hiperplano *(SVM, regresión logística)*  \n",
    "- Distancia de un punto a un plano *(Margen en SVM)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **8. Métodos Numéricos**  \n",
    "8.1 **Aproximación de Funciones**  \n",
    "- Interpolación y extrapolación *(Series temporales, regresión)*  \n",
    "8.2 **Estabilidad Numérica**  \n",
    "- Overflow/underflow *(Softmax, redes neuronales)*  \n",
    "- Normalización de datos *(Preprocesamiento en ML)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **9. Teoría de la Información**  \n",
    "9.1 **Entropía y Ganancia de Información**  \n",
    "- Entropía de Shannon *(Árboles de decisión, ID3)*  \n",
    "- Divergencia de Kullback-Leibler *(EM algorithm, NLP)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **10. Cálculo Tensorial (Básico)**  \n",
    "10.1 **Tensores y Operaciones**  \n",
    "- Producto tensorial *(Redes neuronales profundas, NLP)*  \n",
    "\n",
    "---\n",
    "\n",
    "### **Modelos/Áreas Asociadas**  \n",
    "- **Regresión Lineal**: Álgebra lineal (mínimos cuadrados), cálculo (gradientes).  \n",
    "- **XGBoost**: Optimización (gradiente descendente), árboles (geometría analítica).  \n",
    "- **MILP**: Optimización restringida, programación entera.  \n",
    "- **Redes Neuronales**: Cálculo (regla de la cadena), álgebra lineal (matrices).  \n",
    "\n",
    "--- \n",
    "\n",
    "Este roadmap te dará las bases para entender la matemática detrás de los algoritmos clave. ¡Domina cada capítulo antes de saltar a los modelos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da7468-6dd3-4eaa-8f39-ead26e3a8ffc",
   "metadata": {},
   "source": [
    "## 1. Álgebra lineal \n",
    "\n",
    "### Vectores\n",
    "Los vectores son...\n",
    "\n",
    "### Escalar\n",
    "...\n",
    "\n",
    "### Definición de una ecuación lineal \n",
    "$a_1x_1+a_2x_2+a_3x_3+ \\cdots +a_nx_n = b$\n",
    "\n",
    "donde:\n",
    "- $x_1, x_2, \\ldots, x_n$ son $n$ variables\n",
    "- $a_1, a_2, \\ldots, a_n$ son coeficientes (números reales)\n",
    "- $b$ es el término \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380096e-8017-4ef9-bb1d-1c017851f30d",
   "metadata": {},
   "source": [
    "Aquí tienes una lista completa de los símbolos LaTeX más utilizados en matemáticas para proyectos de machine learning:\n",
    "\n",
    "## Símbolos LaTeX para Machine Learning\n",
    "\n",
    "### Operadores básicos\n",
    "- Suma: `$+$` → $+$\n",
    "- Resta: `$-$` → $-$\n",
    "- Multiplicación: `$\\cdot$` → $\\cdot$ o `$\\times$` → $\\times$\n",
    "- División: `$\\div$` → $\\div$\n",
    "- Fracción: `$\\frac{a}{b}$` → $\\frac{a}{b}$\n",
    "- Potencia: `$x^n$` → $x^n$\n",
    "- Subíndice: `$x_i$` → $x_i$\n",
    "- Raíz cuadrada: `$\\sqrt{x}$` → $\\sqrt{x}$\n",
    "- Raíz n-ésima: `$\\sqrt[n]{x}$` → $\\sqrt[n]{x}$\n",
    "\n",
    "### Símbolos de conjuntos y relaciones\n",
    "- Pertenencia: `$\\in$` → $\\in$, `$\\notin$` → $\\notin$\n",
    "- Subconjunto: `$\\subset$` → $\\subset$, `$\\subseteq$` → $\\subseteq$\n",
    "- Unión: `$\\cup$` → $\\cup$\n",
    "- Intersección: `$\\cap$` → $\\cap$\n",
    "- Conjunto vacío: `$\\emptyset$` → $\\emptyset$\n",
    "- Conjunto de números reales: `$\\mathbb{R}$` → $\\mathbb{R}$\n",
    "- Conjunto de números enteros: `$\\mathbb{Z}$` → $\\mathbb{Z}$\n",
    "- Conjunto de números naturales: `$\\mathbb{N}$` → $\\mathbb{N}$\n",
    "\n",
    "### Comparaciones\n",
    "- Igual: `$=$` → $=$\n",
    "- No igual: `$\\neq$` → $\\neq$\n",
    "- Aproximadamente igual: `$\\approx$` → $\\approx$\n",
    "- Mayor que: `$>$` → $>$\n",
    "- Menor que: `$<$` → $<$\n",
    "- Mayor o igual que: `$\\geq$` → $\\geq$\n",
    "- Menor o igual que: `$\\leq$` → $\\leq$\n",
    "\n",
    "### Cálculo\n",
    "- Derivada: `$\\frac{d}{dx}$` → $\\frac{d}{dx}$\n",
    "- Derivada parcial: `$\\frac{\\partial f}{\\partial x}$` → $\\frac{\\partial f}{\\partial x}$\n",
    "- Integral: `$\\int f(x) dx$` → $\\int f(x) dx$\n",
    "- Integral definida: `$\\int_{a}^{b} f(x) dx$` → $\\int_{a}^{b} f(x) dx$\n",
    "- Límite: `$\\lim_{x \\to a} f(x)$` → $\\lim_{x \\to a} f(x)$\n",
    "- Suma: `$\\sum_{i=1}^{n} x_i$` → $\\sum_{i=1}^{n} x_i$\n",
    "- Producto: `$\\prod_{i=1}^{n} x_i$` → $\\prod_{i=1}^{n} x_i$\n",
    "\n",
    "### Letras griegas (muy usadas en ML)\n",
    "- Alpha: `$\\alpha$` → $\\alpha$\n",
    "- Beta: `$\\beta$` → $\\beta$\n",
    "- Gamma: `$\\gamma$` → $\\gamma$, `$\\Gamma$` → $\\Gamma$\n",
    "- Delta: `$\\delta$` → $\\delta$, `$\\Delta$` → $\\Delta$\n",
    "- Epsilon: `$\\epsilon$` → $\\epsilon$, `$\\varepsilon$` → $\\varepsilon$\n",
    "- Theta: `$\\theta$` → $\\theta$, `$\\Theta$` → $\\Theta$\n",
    "- Lambda: `$\\lambda$` → $\\lambda$, `$\\Lambda$` → $\\Lambda$\n",
    "- Mu: `$\\mu$` → $\\mu$\n",
    "- Pi: `$\\pi$` → $\\pi$, `$\\Pi$` → $\\Pi$\n",
    "- Sigma: `$\\sigma$` → $\\sigma$, `$\\Sigma$` → $\\Sigma$\n",
    "- Phi: `$\\phi$` → $\\phi$, `$\\Phi$` → $\\Phi$\n",
    "- Omega: `$\\omega$` → $\\omega$, `$\\Omega$` → $\\Omega$\n",
    "\n",
    "### Símbolos específicos de ML/Estadística\n",
    "- Esperanza: `$\\mathbb{E}[X]$` → $\\mathbb{E}[X]$\n",
    "- Varianza: `$\\text{Var}(X)$` → $\\text{Var}(X)$\n",
    "- Probabilidad: `$P(X)$` → $P(X)$\n",
    "- Distribución normal: `$\\mathcal{N}(\\mu,\\sigma^2)$` → $\\mathcal{N}(\\mu,\\sigma^2)$\n",
    "- Norma: `$\\|x\\|$` → $\\|x\\|$, `$\\|x\\|_2$` → $\\|x\\|_2$\n",
    "- Transpuesta: `$X^T$` → $X^T$\n",
    "- Producto escalar: `$\\langle x, y \\rangle$` → $\\langle x, y \\rangle$\n",
    "- Argumento máximo: `$\\arg\\max_x f(x)$` → $\\arg\\max_x f(x)$\n",
    "- Argumento mínimo: `$\\arg\\min_x f(x)$` → $\\arg\\min_x f(x)$\n",
    "- Gradiente: `$\\nabla f$` → $\\nabla f$\n",
    "- Infinito: `$\\infty$` → $\\infty$\n",
    "\n",
    "### Matrices y vectores\n",
    "- Vector: `$\\vec{v}$` → $\\vec{v}$ o `$\\mathbf{v}$` → $\\mathbf{v}$\n",
    "- Matriz: \n",
    "`\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "`\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Lógica y conjuntos\n",
    "- Para todo: `$\\forall$` → $\\forall$\n",
    "- Existe: `$\\exists$` → $\\exists$\n",
    "- No existe: `$\\nexists$` → $\\nexists$\n",
    "- Implica: `$\\Rightarrow$` → $\\Rightarrow$\n",
    "- Si y solo si: `$\\Leftrightarrow$` → $\\Leftrightarrow$\n",
    "- Y lógico: `$\\land$` → $\\land$\n",
    "- O lógico: `$\\lor$` → $\\lor$\n",
    "- Negación: `$\\neg$` → $\\neg$\n",
    "\n",
    "### Símbolos de algoritmos y complejidad\n",
    "- Notación Big O: `$O(n)$` → $O(n)$\n",
    "- Notación Theta: `$\\Theta(n)$` → $\\Theta(n)$\n",
    "- Notación Omega: `$\\Omega(n)$` → $\\Omega(n)$\n",
    "\n",
    "### Funciones especiales\n",
    "- Función sigmoide: `$\\sigma(x) = \\frac{1}{1+e^{-x}}$` → $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "- ReLU: `$\\max(0,x)$` → $\\max(0,x)$\n",
    "- Función de pérdida: `$\\mathcal{L}$` → $\\mathcal{L}$\n",
    "- Función de costo: `$J(\\theta)$` → $J(\\theta)$\n",
    "\n",
    "### Puntos suspensivos\n",
    "- En línea horizontal: `$\\ldots$` → $\\ldots$\n",
    "- Centrados: `$\\cdots$` → $\\cdots$\n",
    "- Verticales: `$\\vdots$` → $\\vdots$\n",
    "- Diagonales: `$\\ddots$` → $\\ddots$\n",
    "\n",
    "### Acentos y decoraciones\n",
    "- Sombrero: `$\\hat{x}$` → $\\hat{x}$\n",
    "- Barra: `$\\bar{x}$` → $\\bar{x}$\n",
    "- Tilde: `$\\tilde{x}$` → $\\tilde{x}$\n",
    "- Vector: `$\\vec{x}$` → $\\vec{x}$\n",
    "- Prima (derivada): `$x'$` → $x'$, `$x''$` → $x''$\n",
    "\n",
    "Este conjunto de símbolos te permitirá escribir prácticamente cualquier ecuación necesaria para proyectos de machine learning en Jupyter Notebooks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
